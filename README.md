ğŸï¸ Deep Q-Learning Race Track Agent

A reinforcement learning project where a Deep Q-Network (DQN) agent learns to drive autonomously around a custom 2D race track using ray-based perception and physics-based vehicle dynamics.

This project was built to explore applied deep reinforcement learning, reward shaping, and environment design from scratch.

â¸»

ğŸ¥ Demo

(Add a short GIF or screenshot here later)

The agent:
	â€¢	Uses ray sensors to detect walls
	â€¢	Learns throttle + steering control
	â€¢	Receives reward for forward progress
	â€¢	Completes full laps without checkpoints
	â€¢	Improves lap time through training

â¸»

ğŸ§  Project Overview

This project implements:
	â€¢	Custom 2D race environment (no gym dependency)
	â€¢	Physics-based car model
	â€¢	Ray-cast perception system
	â€¢	Deep Q-Network (PyTorch)
	â€¢	Experience replay buffer
	â€¢	Epsilon-greedy exploration
	â€¢	Reward shaping with lap detection
	â€¢	Training metrics logging

Unlike many RL tutorials, this environment was built entirely from scratch â€” including:
	â€¢	Collision detection
	â€¢	Waypoint-based progress tracking
	â€¢	Start/finish line lap detection
	â€¢	Stuck detection and no-progress termination

â¸»

ğŸ—ï¸ Environment Design

Observation Space

Each state consists of:
	â€¢	13 forward-facing ray distances (normalized 0â€“1)
	â€¢	1 normalized speed value

Total state dimension: 14

Action Space (Discrete: 9 actions)

Action	Description
0	Steer Left
1	Steer Right
2	Throttle Forward
3	Throttle Reverse
4	Forward + Left
5	Forward + Right
6	Reverse + Left
7	Reverse + Right
8	No Input


â¸»

ğŸ¯ Reward Function

Reward is composed of:
	â€¢	âœ… Forward progress along track waypoints
	â€¢	â• Small forward velocity incentive
	â€¢	â– Time penalty per step
	â€¢	â– Wall proximity penalty
	â€¢	â– Crash penalty
	â€¢	ğŸ‰ Lap completion bonus

Progress is calculated using closest waypoint indexing and normalized over total track length.

Lap detection uses start/finish line intersection â€” no artificial checkpoints.

â¸»

ğŸ§ª Training Setup
	â€¢	Algorithm: Deep Q-Network (DQN)
	â€¢	Framework: PyTorch
	â€¢	Device: Apple MPS (Metal GPU acceleration)
	â€¢	Replay Buffer Size: 50,000
	â€¢	Max Steps: 50,000 per training run
	â€¢	Epsilon Decay: Linear
	â€¢	Physics timestep: Fixed 1/60s

Training logs example:

step=50000 eps=0.050 buffer=50000 device=mps laps=5 crashes=187

After training, the agent consistently completes laps autonomously.

Best lap time achieved:

8.35 seconds


â¸»

ğŸ–¥ï¸ How To Run

1ï¸âƒ£ Create virtual environment

python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

2ï¸âƒ£ Train agent

python -m src.train_dqn

3ï¸âƒ£ View trained policy

MODEL_PATH="runs/<timestamp>/dqn_final.pt" python -m src.main_view_policy


â¸»

ğŸ“‚ Project Structure

src/
â”‚
â”œâ”€â”€ env/
â”‚   â”œâ”€â”€ racetrack_env.py
â”‚   â”œâ”€â”€ track.py
â”‚   â”œâ”€â”€ car.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ rl/
â”‚   â”œâ”€â”€ dqn_agent.py
â”‚   â””â”€â”€ replay_buffer.py
â”‚
â”œâ”€â”€ train_dqn.py
â””â”€â”€ main_view_policy.py


â¸»

ğŸ§© Key Engineering Challenges
	â€¢	Stabilizing DQN training
	â€¢	Preventing reward exploitation
	â€¢	Designing smooth progress measurement
	â€¢	Eliminating checkpoint hacks
	â€¢	Avoiding spinning / wall-hugging behavior
	â€¢	Ensuring stable lap detection

â¸»

ğŸš€ Future Improvements
	â€¢	Double DQN
	â€¢	Prioritized replay
	â€¢	Continuous control (DDPG / PPO)
	â€¢	Curved or procedurally generated tracks
	â€¢	Curriculum learning
	â€¢	Model-based RL experiments

â¸»

ğŸ“š What I Learned
	â€¢	How reward shaping dramatically affects agent behavior
	â€¢	Why checkpoint systems can produce shortcut exploitation
	â€¢	The importance of environment design in RL
	â€¢	Debugging unstable Q-value explosions
	â€¢	Practical reinforcement learning beyond textbook examples

â¸»
